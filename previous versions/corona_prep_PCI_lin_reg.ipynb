{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ranking-complex",
   "metadata": {},
   "source": [
    "# Corona prepping using Finnish data regression using OLS regression and the Potential for Change Index (PCI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-albania",
   "metadata": {},
   "source": [
    "## Main question: at this point we're interested in one single regression, i.e. __what predicts whether people do maskless contacts with non-householders__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "systematic-trail",
   "metadata": {},
   "source": [
    "[Research Document](https://docs.google.com/document/d/1iLciHcvVvf8QwFS7wiyNBevpD1B9yDRqMlM4_oCcVcA/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-packet",
   "metadata": {},
   "source": [
    "[Questions codebook](https://docs.google.com/document/d/1YZVCP1UNxnNLAK2kYDfA9Y98leTZYurZD-d8iByhdi0/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-recruitment",
   "metadata": {},
   "source": [
    "[Method of delivery](https://docs.google.com/document/d/1G1JT9JUJrTK3aaXXuRawYACJaGNxU7mcXL9i-d8eKXY/edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "original-rainbow",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'session_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/76/w4hjx50937lb151qm8l0xq_w0000gn/T/ipykernel_6301/3653018926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msession_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m from ml_class import (plot_cv_indices,\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'session_info'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import session_info\n",
    "\n",
    "from ml_class import (plot_cv_indices,\n",
    "                      plot_decision_boundary,\n",
    "                     plot_learning_curve,\n",
    "                     multi_roc_auc_plot,\n",
    "                     dict_of_models,\n",
    "                     RFE_opt_rf,\n",
    "                     make_confusion_matrix,\n",
    "                     summary_performance_metrics_classification)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, GroupKFold, GroupShuffleSplit, RepeatedStratifiedKFold, RepeatedKFold\n",
    "# from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from scipy import stats\n",
    "# from sklearn.feature_selection import RFE, RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-coverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score, cross_validate\n",
    "# from sklearn.ensemble import BaggingClassifier, BaggingRegressor\n",
    "# from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "republican-gazette",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frozen-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriental-michigan",
   "metadata": {},
   "source": [
    "### Virtual Environments and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_info.show(req_file_name=\"corona_preppers-requirements.txt\",\n",
    "      write_req_file=False) #add write_req_file=True to function to get requirements.txt file of packages used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-flood",
   "metadata": {},
   "source": [
    "### Read in data, show info and data head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/shield_gjames_21-09-20_prepped.csv\").drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-separate",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdt_columns = df.filter(regex=\"sdt\").columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_sdt = True\n",
    "if drop_sdt:\n",
    "    df=df.drop(sdt_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-bunny",
   "metadata": {},
   "source": [
    "### Specify the feature list, grouping variable, and specify the grouping variable as a categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divided-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"intention_behavior_composite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[target] = (df[target] - 10) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_list = df.filter(regex=\"^automaticity|attitude|^norms|^risk|^effective\").columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-silly",
   "metadata": {},
   "source": [
    "## EDA on the target\n",
    "Check the amount of samples in the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-alert",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.violinplot(data=df[[target]].melt(), \n",
    "                    x=\"variable\", \n",
    "                    y=\"value\"\n",
    "               )\n",
    "_ = sns.stripplot(data=df[[target]].melt(), \n",
    "                    x=\"variable\", \n",
    "                    y=\"value\",\n",
    "                  edgecolor='white',\n",
    "                  linewidth=0.5\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-manitoba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df[\"demographic_gender\"], df[\"demographic_age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-rebecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = df[target]\n",
    "target_df.describe().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-philippines",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(20, 5))\n",
    "_ = sns.countplot(x=target_df)\n",
    "_ = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-subscription",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (df[[\"demographic_age\", \"demographic_higher_education\"] + features_list + [target]]\n",
    "#  .drop(drop_list, axis=1)\n",
    "#  .assign(target = target_df)\n",
    "#       .dropna(axis=0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-greece",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[target].value_counts().head().to_frame()), df.shape[0], df[target].value_counts().head().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-forth",
   "metadata": {},
   "source": [
    "## Correlations between features and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features_list]\n",
    "y = df[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df = pd.DataFrame()\n",
    "\n",
    "for column in features_list:\n",
    "    temp_corr_df = (pg.corr(x=X.loc[:, column], y=y, method=\"pearson\")\n",
    "                    .reset_index()\n",
    "                    .rename(columns={\"index\": \"type\"})\n",
    "                    .assign(**{\"feature\": column})\n",
    "                    .set_index(\"feature\")\n",
    "                   )\n",
    "    corrs_df = pd.concat([corrs_df, temp_corr_df])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df.sort_values(\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_corrs_features = corrs_df[corrs_df[\"r\"] < 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_corrs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-probability",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in neg_corrs_features:\n",
    "    _ = sns.lmplot(data=df, \n",
    "               x=target, \n",
    "               y=feature, \n",
    "               hue=\"demographic_age\",\n",
    "              legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-defeat",
   "metadata": {},
   "source": [
    "## Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(endog=y, exog=X)\n",
    "res = mod.fit()\n",
    "display(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-ceramic",
   "metadata": {},
   "source": [
    "## Multiple univariate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(endog=y, exog=X[[neg_corrs_features[0]]], hasconst=False)\n",
    "res = mod.fit(method=\"qr\")\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unique-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=True,\n",
    "                        copy_X=True,\n",
    "                        n_jobs=None,\n",
    "                        positive=False)\n",
    "\n",
    "reg = model.fit(X[[neg_corrs_features[0]]], y)\n",
    "\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression(fit_intercept=False,\n",
    "                        copy_X=True,\n",
    "                        n_jobs=None,\n",
    "                        positive=False)\n",
    "\n",
    "reg = model.fit(X[[neg_corrs_features[0]]], y)\n",
    "\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(X[[neg_corrs_features[0]]], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coefs_df = pd.DataFrame()\n",
    "for feature in features_list:\n",
    "    mod = sm.OLS(endog=y, exog=sm.add_constant(X[[feature]]))\n",
    "    res = mod.fit()\n",
    "    coef_df = pd.read_html(res.summary().tables[1].as_html(),header=0,index_col=0)[0]\n",
    "    coef_df = coef_df.assign(**{\"rsquared\": res.rsquared,\n",
    "                                \"rsquared_adj\": res.rsquared_adj})\n",
    "    all_coefs_df = pd.concat([all_coefs_df, coef_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coefs_df.drop(\"const\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handed-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_feature = all_coefs_df.drop(\"const\").sort_values(\"rsquared_adj\").tail(1).iloc[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "knowing-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.lmplot(data=df, \n",
    "               x=target, \n",
    "               y=top_feature, \n",
    "               hue=\"demographic_age\",\n",
    "              legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "centered-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.jointplot(data=df, \n",
    "                  x=target, \n",
    "                  y=top_feature, \n",
    "                  hue=\"demographic_age\",\n",
    "                  # kind=\"reg\",\n",
    "                   legend=True\n",
    "                 )\n",
    "# _ = ax._legend.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coefs_df.drop(\"const\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-dubai",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.boxplot(data=all_coefs_df[[\"rsquared_adj\", \"P>|t|\"]].drop(\"const\").melt(),\n",
    "                x=\"variable\", y=\"value\")\n",
    "_ = plt.axhline(y=0.05, c=\"grey\", ls=\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-summit",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(endog=y, exog=X[[top_feature]])\n",
    "res = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-structure",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = res.predict(exog = X[[top_feature]])\n",
    "\n",
    "df_test = pd.DataFrame({\"y_pred\": y_pred, target: y})\n",
    "\n",
    "user_ids_first = df_test.head(1).index.tolist()[0]\n",
    "user_ids_last = df_test.tail(1).index.tolist()[0]\n",
    "\n",
    "plot_title=\"All\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-aurora",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(30,8))\n",
    "_ = plt.title(f\"Linear Regression(fitted set) | RMSE = {round(np.sqrt(mean_squared_error(df_test['y_pred'], df_test[target])),4)} | bias Error = {round(np.mean(df_test['y_pred'] - df_test[target]), 4)} | {plot_title}\")\n",
    "rmse_plot = plt.stem(df_test.index, df_test['y_pred'] - df_test[target], use_line_collection=True, linefmt='grey', markerfmt='D')\n",
    "_ = plt.hlines(y=round(np.sqrt(mean_squared_error(df_test['y_pred'], df_test[target])),2), colors='b', linestyles='-.', label='+ RMSE', \n",
    "               xmin = user_ids_first, \n",
    "               xmax = user_ids_last\n",
    "              ) \n",
    "_ = plt.hlines(y=round(-np.sqrt(mean_squared_error(df_test['y_pred'], df_test[target])),2), colors='b', linestyles='-.', label='- RMSE', \n",
    "               xmin = user_ids_first, \n",
    "               xmax = user_ids_last\n",
    "              ) \n",
    "_ = plt.xticks(rotation=90, ticks=df_test.index)\n",
    "_ = plt.ylabel(f\"'Error = y_predicted - {target}'\")\n",
    "# _ = plt.ylim([(df_test['y_pred'] - df_test[grouping_var]).min(),\n",
    "#               (df_test['y_pred'] - df_test[grouping_var]).max()])\n",
    "_ = plt.legend()\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-orlando",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_univariate_OLSs(X: pd.DataFrame,\n",
    "                             y: pd.Series,\n",
    "                            features_list: list,):\n",
    "    all_coefs_df = pd.DataFrame()\n",
    "    for feature in features_list:\n",
    "        mod = sm.OLS(endog=y, exog=sm.add_constant(X[[feature]]))\n",
    "        res = mod.fit()\n",
    "        coef_df = pd.read_html(res.summary().tables[1].as_html(),header=0,index_col=0)[0].drop(\"const\")\n",
    "        coef_df = coef_df.assign(**{\"rsquared\": res.rsquared,\n",
    "                                    \"rsquared_adj\": res.rsquared_adj})\n",
    "        all_coefs_df = pd.concat([all_coefs_df, coef_df])\n",
    "    return all_coefs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_dict = {\"18 - 39\": ['18-29','30-39'],\n",
    "              \"40 - 59\": ['40-49', '50-59'],\n",
    "              \"60+\": ['60+'],\n",
    "              \"All\": ['60+', '40-49', '18-29', '50-59', '30-39'],\n",
    "              \"Lower Education\": 0,\n",
    "              \"Higher Education\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_ols_df[[\"coef\", \"P>|t|\", \"rsquared\", \"rsquared_adj\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ols_df = pd.DataFrame()\n",
    "for group in groups_dict:\n",
    "    if type(groups_dict[group]) == list:\n",
    "        tmp_df = df[df[\"demographic_age\"].isin(groups_dict[group])]\n",
    "    else:\n",
    "        tmp_df = df[df[\"demographic_higher_education\"] == groups_dict[group]]\n",
    "        \n",
    "    tmp_X = tmp_df[features_list]\n",
    "    tmp_y = tmp_df[target]\n",
    "\n",
    "    tmp_ols_df = multiple_univariate_OLSs(X=tmp_X, \n",
    "                                          y=tmp_y, \n",
    "                                          features_list=features_list)[[\"coef\", \"P>|t|\", \"rsquared_adj\"]]\n",
    "    tmp_ols_df.columns = pd.MultiIndex.from_tuples([(group, x) for x in tmp_ols_df.columns.tolist()])\n",
    "    all_ols_df = pd.concat([all_ols_df, tmp_ols_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ols_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(20,10))\n",
    "_ = sns.heatmap(data=all_ols_df.sort_values(by = (\"All\", \"coef\"), ascending=False),\n",
    "               annot=True)\n",
    "_ = plt.xlabel(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confused-validation",
   "metadata": {},
   "source": [
    "So, the sample-level PCI is simply:\n",
    "\n",
    "[Room for improvement] * [Weight]\n",
    "\n",
    "Room for improvement is the distance of some centrality measure from a minimum or maximum value. The centrality measure we use in the tables in the analysis script is the mean; the minimum and maximum are the observed minimum and maximum. (you could also use e.g. scale min/max; or trimmed min/max; and you can also use median or trimmed mean etc). Whether you take the distance from the minimum or maximum depends on whether the determinant is positively or negatively associated to the criterion/target. If it's associated positively, you take the maximum (because 'increasing' the determinant increases the target), and if it's associated negatively, you take the minimum (because 'decreasing' the determinant increases the target).\n",
    "\n",
    "The Weight is some indication of the strength of the association between the determinant and the criterion/target; e.g. the correlation or the proportion of explained variance.\n",
    "\n",
    "The manual for the R function in the behaviorchange package is at https://r-packages.gitlab.io/behaviorchange/reference/potential_for_change.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-black",
   "metadata": {},
   "source": [
    "## All of the coefficients are positive hence the maximum is used as the extremity_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-breath",
   "metadata": {},
   "source": [
    "## because the distributions of the features and the target are scewed the median is used as the centrality_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-cemetery",
   "metadata": {},
   "source": [
    "### trimmed_mean = remove 2.5% up and bottom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from typing import Callable, Dict, Union\n",
    "\n",
    "def apply_scaling(df: pd.DataFrame, \n",
    "                  method: Union[Callable, str] = \"MinMax\", \n",
    "                  kwargs: Dict = {}):\n",
    "    if method == \"MinMax\":\n",
    "        scal_df = pd.DataFrame(MinMaxScaler(**kwargs).fit_transform(df), \n",
    "             index = df.index,\n",
    "            columns = df.columns)\n",
    "    elif method == \"Standard\":\n",
    "        scal_df = pd.DataFrame(StandardScaler(**kwargs).fit_transform(df), \n",
    "             index = df.index,\n",
    "            columns = df.columns)\n",
    "    else:\n",
    "        scal_df = pd.DataFrame(method(**kwargs).fit_transform(df), \n",
    "             index = df.index,\n",
    "            columns = df.columns)\n",
    "    return scal_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.tmean(df[features_list[0]], limits=(1, 7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesbian-rough",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[features_list].agg(stats.tmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[features_list].agg(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_measure = \"mean\"\n",
    "extremity_measure = None\n",
    "weight_measure = \"r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "professional-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The chosen parameters to calculate the PCI are:\\n- centrality_measure = {centrality_measure}\\n- extremity_measure = {extremity_measure}\\n- weight_measure = {weight_measure}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[features_list].agg(extremity_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-kennedy",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_coeffs_list = all_coefs_df[all_coefs_df[\"coef\"] < 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_coefs_df.loc[negative_coeffs_list, weight_measure]#.drop(\"const\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_corrs_list = corrs_df[corrs_df[\"r\"] < 0].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-cooperative",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df.loc[negative_corrs_list, weight_measure]#.drop(\"const\", axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(negative_coeffs_list) < 0:\n",
    "    \n",
    "    pci_df = (\n",
    "        # room for improvement calculation (series)\n",
    "        (df[features_list]\n",
    "         .pipe(apply_scaling)\n",
    "         .agg(centrality_measure)\n",
    "         - (df[features_list]\n",
    "            .pipe(apply_scaling)\n",
    "            .agg(\"max\"))\n",
    "        ).abs()\n",
    "\n",
    "        *\n",
    "\n",
    "        # weight (based on rsqaured_adj series)\n",
    "\n",
    "        all_coefs_df[weight_measure]\n",
    "\n",
    "    ).to_frame(\"PCI\")\n",
    "else:\n",
    "    neg_pci_df = (\n",
    "        # room for improvement calculation (series)\n",
    "        (df[negative_coeffs_list]\n",
    "         .pipe(apply_scaling)\n",
    "         .agg(centrality_measure, axis=1)\n",
    "         - (df[negative_coeffs_list]\n",
    "            .pipe(apply_scaling)\n",
    "            .agg(\"min\"))\n",
    "        ).abs()\n",
    "\n",
    "        *\n",
    "\n",
    "        # weight (based on rsqaured_adj series)\n",
    "\n",
    "#         all_coefs_df.loc[negative_coeffs_list, weight_measure]\n",
    "        corrs_df.loc[negative_corrs_list, weight_measure]\n",
    "\n",
    "    ).to_frame(\"PCI\")\n",
    "    \n",
    "    pos_pci_df = (\n",
    "        # room for improvement calculation (series)\n",
    "        (df[features_list].drop(negative_coeffs_list, axis=1)\n",
    "         .pipe(apply_scaling)\n",
    "         .agg(centrality_measure)\n",
    "         - (df[features_list]\n",
    "            .drop(negative_coeffs_list, axis=1)\n",
    "            .pipe(apply_scaling)\n",
    "            .agg(\"max\"))\n",
    "        ).abs()\n",
    "\n",
    "        *\n",
    "\n",
    "        # weight (based on rsqaured_adj series)\n",
    "\n",
    "#         all_coefs_df[weight_measure].drop(negative_coeffs_list + [\"const\"], axis=0)\n",
    "        corrs_df[weight_measure].drop(negative_corrs_list, axis=0)\n",
    "\n",
    "    ).to_frame(\"PCI\")\n",
    "    \n",
    "    pci_df = pd.concat([pos_pci_df, neg_pci_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-light",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(2, 8))\n",
    "_ = sns.heatmap(data=pci_df.dropna().sort_values(\"PCI\", ascending=False),\n",
    "                annot=True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def potential_for_change_index(data: pd.DataFrame,\n",
    "                               features_list: list,\n",
    "                              centrality_measure: str = \"median\",\n",
    "                               # extremity_measure: str = \"max\",\n",
    "                               weight_measure: str = \"rsquared_adj\"):\n",
    "    \n",
    "    tmp_X = data[features_list]\n",
    "    tmp_y = data[target]\n",
    "\n",
    "    ols_df = multiple_univariate_OLSs(X=tmp_X, \n",
    "                                          y=tmp_y, \n",
    "                                          features_list=features_list)\n",
    "    \n",
    "    negative_coeffs_list = ols_df[ols_df[\"coef\"] < 0].index.tolist()\n",
    "    \n",
    "    if len(negative_coeffs_list) < 0:\n",
    "    \n",
    "        pci_df = (\n",
    "            # room for improvement calculation (series)\n",
    "            (data[features_list]\n",
    "             .pipe(apply_scaling)\n",
    "             .agg(centrality_measure)\n",
    "             - (data[features_list]\n",
    "                .pipe(apply_scaling)\n",
    "                .agg(\"max\"))\n",
    "            ).abs()\n",
    "\n",
    "            *\n",
    "\n",
    "            # weight (based on rsqaured_adj series)\n",
    "\n",
    "#             all_coefs_df[weight_measure]\n",
    "            corrs_df[weight_measure]\n",
    "\n",
    "        ).to_frame(\"PCI\")\n",
    "    else:\n",
    "        neg_pci_df = (\n",
    "            # room for improvement calculation (series)\n",
    "            (data[negative_coeffs_list]\n",
    "             .pipe(apply_scaling)\n",
    "             .agg(centrality_measure)\n",
    "             - (data[negative_coeffs_list]\n",
    "                .pipe(apply_scaling)\n",
    "                .agg(\"min\"))\n",
    "            ).abs()\n",
    "\n",
    "            *\n",
    "\n",
    "            # weight (based on rsqaured_adj series)\n",
    "\n",
    "#             all_coefs_df.loc[negative_coeffs_list, weight_measure]\n",
    "            corrs_df.loc[negative_corrs_list, weight_measure]\n",
    "\n",
    "        ).to_frame(\"PCI\")\n",
    "\n",
    "        pos_pci_df = (\n",
    "            # room for improvement calculation (series)\n",
    "            (data[features_list].drop(negative_coeffs_list, axis=1)\n",
    "             .pipe(apply_scaling)\n",
    "             .agg(centrality_measure)\n",
    "             - (data[features_list].drop(negative_coeffs_list, axis=1)\n",
    "                .pipe(apply_scaling)\n",
    "                .agg(\"max\"))\n",
    "            ).abs()\n",
    "\n",
    "            *\n",
    "\n",
    "            # weight (based on rsqaured_adj series)\n",
    "\n",
    "#             all_coefs_df[weight_measure].drop(negative_coeffs_list + [\"const\"], axis=0)\n",
    "            corrs_df[weight_measure].drop(negative_corrs_list, axis=0)\n",
    "\n",
    "        ).to_frame(\"PCI\")\n",
    "\n",
    "        pci_df = pd.concat([pos_pci_df, neg_pci_df])\n",
    "        \n",
    "    \n",
    "    return pci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-albert",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_sample_size = 1000\n",
    "bootstrap_number = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indian-german",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pcis_df = pd.DataFrame()\n",
    "for i in range(0, bootstrap_number):\n",
    "    # print(df.sample(n=bootstrap_sample_size, random_state=0 + i).index)\n",
    "    tmp_pci_df = potential_for_change_index(data=df.sample(n=bootstrap_sample_size, random_state=0 + i),\n",
    "                               features_list=features_list,\n",
    "                              centrality_measure = centrality_measure,\n",
    "                               # extremity_measure = \"max\",\n",
    "                               weight_measure = weight_measure)\n",
    "    all_pcis_df = pd.concat([all_pcis_df, tmp_pci_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pcis_df.columns = [f\"PCI_{x}\" for x in range(0, all_pcis_df.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(6, 8))\n",
    "_ = sns.heatmap(all_pcis_df.sort_values(by=\"PCI_0\", ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-startup",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.figure(figsize=(6, 8))\n",
    "_ = sns.heatmap(all_pcis_df.agg([\"min\", \"mean\", \"median\", \"max\"], axis=1).sort_values(by=\"mean\", ascending=False),\n",
    "               annot=True,\n",
    "               fmt=\".3g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pcis_df = pd.DataFrame()\n",
    "for group in groups_dict:\n",
    "    if type(groups_dict[group]) == list:\n",
    "        tmp_df = df[df[\"demographic_age\"].isin(groups_dict[group])]\n",
    "    else:\n",
    "        tmp_df = df[df[\"demographic_higher_education\"] == groups_dict[group]]\n",
    "\n",
    "    tmp_pci_df = potential_for_change_index(data=tmp_df,\n",
    "                               features_list=features_list,\n",
    "                              centrality_measure = centrality_measure,\n",
    "                               # extremity_measure = \"max\",\n",
    "                               weight_measure = weight_measure)\n",
    "    tmp_pci_df.columns = [f\"PCI_{group}\"]\n",
    "    all_pcis_df = pd.concat([all_pcis_df, tmp_pci_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-failing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pcis_df\n",
    "_ = plt.figure(figsize=(6, 8))\n",
    "_ = sns.heatmap(all_pcis_df.sort_values(by=\"PCI_All\", ascending=False),\n",
    "               annot=True,\n",
    "                fmt=\".3g\"\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-weather",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_env",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
